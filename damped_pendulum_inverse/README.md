# Damped Pendulum Inverse Problem - Parameter Inference (Level 3)

**Complexity:** ‚≠ê‚≠ê‚≠ê ADVANCED  
**Equation:** `Œ∏'' + Œ∂¬∑Œ∏' + (g/l)¬∑sin(Œ∏) = 0` (Œ∂ unknown)  
**System Type:** Inverse problem - learn unknown parameter from observations

---

## üìñ Problem Description

### Physics
This is the most advanced problem: **learn unknown system parameters from noisy observations**.

**Governing Equation:**
$$\frac{d^2\theta}{dt^2} + \zeta\frac{d\theta}{dt} + \frac{g}{l}\sin(\theta) = 0$$

Where:
- **Œ∏(t):** Angular displacement (observed, noisy)
- **Œ∂ (zeta):** Damping ratio (**UNKNOWN** - must be inferred!)
- **g/l:** Gravitational coefficient (9.81 m/s¬≤)

### The Inverse Problem
**Given:**
- Noisy observations: Œ∏_obs(t) = Œ∏_true(t) + Œµ

**Find:**
- Unknown parameter: Œ∂ ‚àà [0, 5]
- True trajectory: Œ∏(t)

**Constraints:**
- Physics: ODE residual must be small
- Data fidelity: Prediction must match observations
- Smoothness: Solution should be regular

### Initial Conditions
- **Œ∏(0) = œÄ/3 rad** (60¬∞ initial angle)
- **dŒ∏/dt(0) = 0 rad/s** (released from rest)
- **Œ∂_true = 0.5** (ground truth for validation)

### Simulation Parameters
| Parameter | Value | Meaning |
|-----------|-------|---------|
| Unknown damping (Œ∂) | 0.5 | Need to infer this! |
| Initial angle (Œ∏‚ÇÄ) | œÄ/3 (60¬∞) | Moderate nonlinearity |
| Simulation time | 10 seconds | ~3-4 oscillations with decay |
| Observation noise | 1-5% Gaussian | Realistic measurement error |
| Observation points | 100-200 | Sparse sampling |

---

## üß† Dual-Network Architecture

### Two Neural Networks

#### Network 1: Forward Solution Network
```
Input: time t_norm ‚àà [0,1]
    ‚Üì
256 neurons √ó 6 layers (tanh)
    ‚Üì
Output: angle Œ∏(t)
```

**Purpose:** Predicts trajectory Œ∏(t)

#### Network 2: Parameter Network
```
Input: 1 (constant, shared encoding)
    ‚Üì
128 neurons √ó 3 layers (tanh)
    ‚Üì
Output: scalar Œ∂ (damping ratio)
```

**Purpose:** Predicts unknown parameter Œ∂

### Total Parameters
- Forward network: ~145,000
- Parameter network: ~15,000
- **Total:** ~160,000 (modest, joint training)

---

## ‚öôÔ∏è Two-Phase Training Strategy

### Phase 1: Encoder Network (Initial Conditions)
**Purpose:** Learn which initial conditions best match observed data
**Time:** ~20 minutes
**Loss:** Data fidelity only (no physics yet)

```
Domain: Auxiliary encoder network
Input: Partial trajectory
Output: Predicted IC (Œ∏‚ÇÄ, dŒ∏/dt|‚ÇÄ)
Loss: L_encoder = ||Œ∏_pred - Œ∏_obs||¬≤
```

**Output:** Pretrained IC predictor weights

### Phase 2: Inverse Problem Solver
**Purpose:** Learn both trajectory AND unknown parameter
**Time:** ~40 minutes
**Loss:** Physics + data constraints

```
Domain: Forward + Parameter networks
Inputs: Time t
Outputs: Œ∏(t), Œ∂
Constraints:
  1. ODE residual: ||Œ∏'' + Œ∂Œ∏' + (g/l)sin(Œ∏)||
  2. Data fit: ||Œ∏_pred - Œ∏_obs||¬≤
  3. Initial conditions: Œ∏(0) = Œ∏‚ÇÄ, Œ∏'(0) = 0
Loss: L_total = w_ODE¬∑L_ODE + w_data¬∑L_data + w_IC¬∑L_IC
```

**Output:** Trained forward network + parameter network

---

## üîß How to Run

### Setup
```bash
cd damped_pendulum_inverse/
conda activate physicsnemo
```

### Phase 1: Train Encoder
```bash
# Learn initial conditions from full trajectory
python damped_pendulum_encoder_solver.py

# Output: outputs/damped_pendulum_encoder_solver/
# Time: ~20 minutes
```

### Phase 2: Inverse Problem
```bash
# Learn both trajectory and unknown Œ∂
python damped_pendulum_inverse_solver.py

# Output: outputs/damped_pendulum_inverse_solver/
# Time: ~40 minutes
# Total: ~1 hour
```

### Visualization
```bash
# Phase 1 results
python plot_results_encoder.py
# Shows: Predicted vs actual IC, encoder reconstruction

# Phase 2 results
python plot_results_inverse.py
# Shows: Inferred Œ∂, trajectory comparison, phase space
```

### Configuration Override
```bash
# Different observation noise level (Phase 2)
python damped_pendulum_inverse_solver.py \
  noise_level=0.05 \
  training.max_steps=75000

# Different network size
python damped_pendulum_inverse_solver.py \
  arch.fully_connected.layer_size=512
```

---

## üìä Expected Results

### Phase 1: Encoder Accuracy
```
‚úì Predicted IC close to actual: Œ∏‚ÇÄ_pred = Œ∏‚ÇÄ_true ¬± 0.05 rad
‚úì Encoder trajectory reconstruction: RMSE < 0.1 rad
‚úì Training loss converges smoothly
```

### Phase 2: Parameter Inference
```
‚úì Œ∂ inference: Œ∂_pred = Œ∂_true ¬± 0.05 (1% noise)
‚úì Œ∂ inference: Œ∂_pred = Œ∂_true ¬± 0.10 (5% noise)
‚úì Trajectory RMSE: 0.05-0.15 rad
‚úì Phase error: < ¬±5¬∞
```

### Accuracy by Noise Level
| Noise | Œ∂ Error | Œ∏ RMSE | Difficulty |
|-------|---------|--------|-----------|
| 0% (synthetic) | ¬±0.01 | 0.02 | Easiest |
| 1% | ¬±0.03 | 0.05 | Easy |
| 5% | ¬±0.08 | 0.12 | Medium |
| 10% | ¬±0.15 | 0.25 | Hard |

---

## üéØ Inverse Problem Challenges

### Challenge 1: Non-Identifiability
**Problem:** Different (Œ∏, Œ∂) pairs may fit same data equally well

**Example:**
- (Œ∏ with Œ∂=0.3, more oscillations) vs
- (Œ∏ with Œ∂=0.5, fewer oscillations)

Both could match sparse noisy observations!

**Solution:**
- Use physics constraints (ODE residual)
- Add regularization on Œ∂
- Use long time horizon (more oscillations)
- Multiple observation frequencies help

### Challenge 2: Local Minima
**Problem:** Optimization may get stuck in local minimum

**Solution:**
- Good initial guess for Œ∂ (e.g., Œ∂=0.5)
- Multi-phase training (forward first)
- Higher learning rates
- Larger networks

### Challenge 3: Noise Sensitivity
**Problem:** Parameter inference sensitive to measurement noise

**Solution:**
- Smooth observations (moving average filter)
- Use more observation points
- Add regularization term
- Uncertainty quantification

### Challenge 4: Multiple Parameters
**Problem:** If multiple unknowns (not just Œ∂), problem becomes harder

**Fundamental limit:** Number of observations must exceed number of unknowns

---

## üî¥ Troubleshooting

### Problem: Œ∂ Inference Way Off
**Symptoms:** Œ∂_pred ‚â† Œ∂_true (e.g., 0.1 vs 0.5)

**Root Cause:** Non-identifiability, poor initial guess, or noisy data

**Solutions:**
1. Ensure observation data is smooth (filter noise)
2. Provide better initial guess: `zeta_init=0.5`
3. Use more observation points (100 ‚Üí 300)
4. Add regularization: `lambda_zeta=1000`
5. Longer simulation time (10s ‚Üí 20s)
6. Higher learning rate (0.001 ‚Üí 0.005)

### Problem: Phase 1 Encoder Doesn't Converge
**Symptoms:** IC prediction error doesn't decrease

**Root Cause:** Encoder network too small or learning rate too low

**Solutions:**
1. Increase network size: `layer_size: 256 ‚Üí 512`
2. Increase encoder steps: `max_steps: 30000 ‚Üí 50000`
3. Higher learning rate: `lr: 0.001 ‚Üí 0.003`
4. Use more IC training points
5. Verify encoder uses pretrained Phase 1 weights

### Problem: Phase 2 Diverges After Phase 1
**Symptoms:** Loss increases after loading Phase 1 weights

**Root Cause:** Learning rate too high, weights not properly initialized

**Solutions:**
1. Lower learning rate: `lr: 0.001 ‚Üí 0.0005`
2. Check encoder weights loaded correctly
3. Verify IC constraints from Phase 1 are applied
4. Use smaller learning rate decay
5. Frozen encoder weights first 5000 steps

### Problem: Trajectory Doesn't Match Observations
**Symptoms:** PINN trajectory deviates from data

**Root Cause:** Data weight too low or ODE weight too high

**Solutions:**
1. Increase data fidelity weight: `lambda_data: 100 ‚Üí 1000`
2. Decrease ODE weight: `lambda_ODE: 1 ‚Üí 0.1`
3. Check observation data is valid (no outliers)
4. Use observation uncertainty weighting
5. Longer training: `max_steps: 50000 ‚Üí 75000`

---

## üìà Understanding Output Files

### Training Outputs (Phase 1)
```
outputs/damped_pendulum_encoder_solver/
‚îú‚îÄ‚îÄ .hydra/
‚îÇ   ‚îî‚îÄ‚îÄ config_encoder.yaml         # Phase 1 configuration
‚îú‚îÄ‚îÄ checkpoints/
‚îÇ   ‚îú‚îÄ‚îÄ epoch_0000.pt               # Encoder checkpoints
‚îÇ   ‚îî‚îÄ‚îÄ epoch_0060.pt (final)
‚îú‚îÄ‚îÄ damped_pendulum_encoder_solver_output.npz
‚îÇ   ‚îú‚îÄ‚îÄ t: Time points
‚îÇ   ‚îú‚îÄ‚îÄ theta_obs: Observed trajectory
‚îÇ   ‚îú‚îÄ‚îÄ theta_pred: Encoder reconstruction
‚îÇ   ‚îú‚îÄ‚îÄ IC_pred: Predicted initial conditions
‚îÇ   ‚îî‚îÄ‚îÄ loss_history: Training loss
‚îî‚îÄ‚îÄ training_logs.txt
```

### Training Outputs (Phase 2)
```
outputs/damped_pendulum_inverse_solver/
‚îú‚îÄ‚îÄ .hydra/
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml                 # Phase 2 configuration
‚îú‚îÄ‚îÄ checkpoints/
‚îÇ   ‚îú‚îÄ‚îÄ epoch_0000.pt               # Forward network
‚îÇ   ‚îú‚îÄ‚îÄ epoch_0000_param.pt         # Parameter network
‚îÇ   ‚îî‚îÄ‚îÄ epoch_0100.pt (final)
‚îú‚îÄ‚îÄ damped_pendulum_inverse_solver_output.npz
‚îÇ   ‚îú‚îÄ‚îÄ t: Time points
‚îÇ   ‚îú‚îÄ‚îÄ theta_pred: Forward network prediction
‚îÇ   ‚îú‚îÄ‚îÄ theta_obs: Observed trajectory
‚îÇ   ‚îú‚îÄ‚îÄ zeta_pred: Inferred damping ratio
‚îÇ   ‚îú‚îÄ‚îÄ zeta_std: Uncertainty (if available)
‚îÇ   ‚îî‚îÄ‚îÄ loss_components: IC, ODE, data losses
‚îî‚îÄ‚îÄ training_logs.txt
```

### Result Visualizations
```
outputs/damped_pendulum_encoder_solver/
‚îú‚îÄ‚îÄ encoder_ic_prediction.png       # Predicted vs actual IC
‚îú‚îÄ‚îÄ encoder_trajectory.png          # Reconstruction accuracy
‚îî‚îÄ‚îÄ encoder_convergence.png         # Loss over training

outputs/damped_pendulum_inverse_solver/
‚îú‚îÄ‚îÄ inferred_parameter.png          # Œ∂_pred vs Œ∂_true
‚îú‚îÄ‚îÄ trajectory_comparison.png       # PINN vs observations
‚îú‚îÄ‚îÄ phase_space.png                 # Phase portrait
‚îú‚îÄ‚îÄ parameter_convergence.png       # Œ∂ evolution over training
‚îî‚îÄ‚îÄ loss_components.png             # ODE, data, IC losses
```

---

## üî¨ Physics Insights

### Damping Identification
The damping ratio Œ∂ affects:
- **Oscillation frequency:** œâd = œâ‚ÇÄ‚àö(1-Œ∂¬≤) (lower with more damping)
- **Decay rate:** e^(-Œ∂œâ‚ÇÄt) (faster with more damping)
- **Quality factor:** Q = 1/(2Œ∂) (oscillations per decay time)

Network must learn these relationships from observation data.

### Observable vs Unobservable
**Observable:** Œ∂ directly affects what we see
- Oscillation frequency changes
- Decay envelope changes
- Easy to infer from full trajectory

**Unobservable:** Some parameters don't affect dynamics
- Example: gravity in small-angle limit (sin(Œ∏) ‚âà Œ∏)
- Need additional information to identify

**Our case:** Œ∂ is highly observable!

### Why Two-Phase Training?

**Phase 1 benefits:**
- IC network provides good initial guess for Œ∏(0), Œ∏'(0)
- Easier to learn ICs separately before adding parameter inference
- Faster convergence in Phase 2

**Phase 2 benefits:**
- Uses Phase 1 weights as initialization
- Focuses on finding Œ∂ with good trajectory guess
- Physics constraints (ODE) guide parameter learning

### Identifiability Condition
For unique Œ∂ inference, need:
- Observation time > 2œÄ/œâd (at least 1-2 complete periods)
- Enough observation points (>50)
- Low noise level (<5%)
- Initial angle moderate (nonlinear but not extreme)

---

## üìö References

### Inverse Problems
- **Tarantola (2005):** "Inverse Problem Theory" (classical reference)
- **Kaipio & Somersalo (2005):** "Statistical and Computational Inverse Problems"
- **Vogel (2002):** "Computational Methods for Inverse Problems"

### PINN for Inverse Problems
- **Raissi et al. (2019):** Original PINN paper (includes inverse)
- **Jagtap et al. (2020):** "Conservative Physics-Informed Neural Networks"

### System Identification
- **Ljung (1999):** "System Identification: Theory for the User"
- **Soderstrom & Stoica (1989):** "System Identification"

---

## ‚úÖ Validation Checklist

### Phase 1 (Encoder)
- [ ] Encoder loss converges below 0.01
- [ ] Predicted IC close to actual: error < 0.05 rad
- [ ] Encoder trajectory reconstruction: RMSE < 0.1 rad
- [ ] Both IC components learned (not just Œ∏)

### Phase 2 (Inverse Problem)
- [ ] ODE residual small (< 1e-3)
- [ ] Data fidelity error < observation noise level
- [ ] Œ∂ inference within ¬±0.05 of true value (1% noise)
- [ ] Phase space matches expected spiral
- [ ] Trajectory passes near observation points
- [ ] Parameter network output is smooth
- [ ] Results reproducible with different random seeds

---

## üí° Tips for Practitioners

### Quick Start Strategy
1. Generate clean synthetic data with known Œ∂_true
2. Run Phase 1 (encoder) - verify IC prediction works
3. Run Phase 2 (inverse) - verify Œ∂ inference
4. Add noise gradually (1% ‚Üí 5% ‚Üí 10%)
5. Finally try real noisy observations

### Debugging Workflow
1. Check if Phase 1 encoder works at all
2. Verify observation data shape and scale
3. Print Œ∂_pred every 100 steps to see convergence
4. Compare ODE, IC, and data loss components
5. Plot loss curves - should be smooth and decreasing

### Hyperparameter Tuning
- **Œ∂ not converging?** Increase data weight, use longer horizon
- **Diverges after Phase 1?** Lower Phase 2 learning rate
- **Encoder doesn't learn ICs?** Use larger encoder network
- **Slow convergence?** Higher learning rate, more training steps

### Noise Handling
- **Clean synthetic data:** Should easily get Œ∂ error < 0.01
- **1% noise:** Expect Œ∂ error ¬±0.03-0.05
- **5% noise:** Expect Œ∂ error ¬±0.08-0.10
- **10% noise:** Expect Œ∂ error ¬±0.15-0.20 (approach limit)

### Multiple Parameters
If inferring Œ∂ and œâ‚ÇÄ jointly:
- Much harder problem (more nonuniqueness)
- Need longer observations
- Parameter network needs more capacity
- Consider sequential identification (Phase 1 Œ∂, Phase 2 œâ‚ÇÄ)

---

## üéì Learning Outcomes

After completing this project, you understand:

- ‚úì Inverse problem formulation & parameter inference
- ‚úì Non-identifiability challenges in inverse problems
- ‚úì Multi-phase training strategies
- ‚úì Hybrid physics + data constraints
- ‚úì Dual-network architectures
- ‚úì Real-world system identification workflow
- ‚úì Noise sensitivity and regularization
- ‚úì Practical debugging techniques

---

**Status:** ‚úì Complete implementation with two-phase training  
**Last Updated:** January 28, 2026  
**Difficulty:** Advanced (integrate concepts from Levels 1 & 2)

---

## üöÄ Recommended Project Extensions

1. **Multiple unknowns:** Infer Œ∂ AND œâ‚ÇÄ simultaneously
2. **Structured uncertainty:** Use Bayesian PINN for confidence intervals
3. **Real data:** Try with actual pendulum measurements
4. **Time-varying parameters:** Learn Œ∂(t) that changes over time
5. **Coupled systems:** Two pendulums with unknown coupling
6. **Hybrid models:** Some terms known (gravity), others unknown (friction)
        "theta_series": torch.tensor(theta_new, dtype=torch.float32)
    })
    
print(f"Estimated Œ∂: {output['zeta'].item():.4f}")
print(f"Estimated Œ∏‚ÇÄ: {output['theta0'].item():.4f}")
print(f"Estimated œâ‚ÇÄ: {output['omega0'].item():.4f}")
```

## Loss Functions

The training minimizes a combined loss:

$$\mathcal{L} = \lambda_\text{data} \mathcal{L}_\text{data} + \lambda_\text{physics} \mathcal{L}_\text{ODE} + \lambda_\text{IC} \mathcal{L}_\text{IC}$$

Where:
- **Data loss:** $\mathcal{L}_\text{data} = \sum_i |\theta_\text{pred}(t_i) - \theta_\text{obs}(t_i)|^2$
- **Physics loss:** $\mathcal{L}_\text{ODE} = \sum_j |R(t_j)|^2$ (ODE residual)
- **IC loss:** $\mathcal{L}_\text{IC} = |\theta(0) - \theta_0|^2 + |\dot\theta(0) - \omega_0|^2$

## Tips for Best Results

1. **Noise sensitivity:** Increase `lambda_physics` if data is noisy
2. **Parameter ranges:** Ensure test cases fall within training ranges
3. **Training time:** Encoder approach needs more training steps (~30k+)
4. **Embedding dimension:** Larger embeddings capture more complex dynamics
5. **Time points:** More observation points improve parameter estimation
