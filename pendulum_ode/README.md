# Pendulum ODE - Physics-Informed Neural Network (Level 1)

**Complexity:** ‚≠ê BEGINNER  
**Equation:** `Œ∏'' + (g/l)¬∑sin(Œ∏) = 0`  
**System Type:** Autonomous Hamiltonian system (energy-conserving)

---

## üìñ Problem Description

### Physics
This is the classical **mathematical pendulum** - a point mass suspended by a rigid, massless rod under gravity.

**Governing Equation:**
$$\frac{d^2\theta}{dt^2} + \frac{g}{l}\sin(\theta) = 0$$

Where:
- **Œ∏(t):** Angular displacement from vertical (radians)
- **g:** Gravitational acceleration (9.81 m/s¬≤)
- **l:** Length of pendulum (1.0 m)
- **d¬≤Œ∏/dt¬≤:** Angular acceleration
- **sin(Œ∏):** Nonlinear restoring force (gravity component)

### Energy Conservation
Total mechanical energy is constant:
$$E = \frac{1}{2}l^2\left(\frac{d\theta}{dt}\right)^2 + gl(1 - \cos\theta) = \text{constant}$$

Kinetic energy oscillates with potential energy - perfect for PINN validation.

### Initial Conditions
- **Œ∏(0) = œÄ/6 rad** (30¬∞ initial angle)
- **dŒ∏/dt(0) = 0 rad/s** (released from rest)

### Simulation Parameters
| Parameter | Value | Meaning |
|-----------|-------|---------|
| Gravitational constant (g) | 9.81 m/s¬≤ | Earth gravity |
| Rod length (l) | 1.0 m | 1 meter pendulum |
| Initial angle (Œ∏‚ÇÄ) | œÄ/6 (30¬∞) | Moderate angle, nonlinear regime |
| Simulation time | 10 seconds | ~5 complete oscillations |
| Period (approx) | 2.0 seconds | 2œÄ‚àö(l/g) ‚âà 2.0s |

---

## üß† PINN Architecture

### Network Structure
```
Input Layer:    1 neuron (time t)
                    ‚Üì
Hidden Layer 1: 128 neurons (tanh)
Hidden Layer 2: 128 neurons (tanh)
...
Hidden Layer 8: 128 neurons (tanh)
                    ‚Üì
Output Layer:   1 neuron (angle Œ∏)
```

**Total Parameters:** ~134,000

### Network Design Rationale
- **128 hidden units:** Sufficient capacity for nonlinear oscillatory behavior
- **8 hidden layers:** Deep enough for complex temporal dependencies
- **tanh activation:** Better for periodic functions than ReLU; smooth derivatives crucial for ODE constraints
- **Single output:** Direct prediction of Œ∏(t)

---

## ‚öôÔ∏è Training Configuration

### Hyperparameters
| Parameter | Value | Purpose |
|-----------|-------|---------|
| Initial learning rate | 0.002 | Fast early convergence |
| LR decay factor | 0.95 per 1500 steps | Prevents instability |
| Training steps | 25,000 | ~15-20 min on GPU |
| Batch size (IC) | 200 | Strong initial condition enforcement |
| Batch size (interior) | 1,000 | ODE constraint coverage |
| Supervised batch | 200 | Truth data points |

### Loss Function Components

The total loss combines:

1. **Initial Condition Loss** (weight: 10,000)
   - Forces Œ∏(0) = œÄ/6 and dŒ∏/dt(0) = 0
   - Most critical constraint (quadratic weight)

2. **ODE Residual Loss** (weight: 5,000)
   - Enforces `Œ∏'' + (g/l)¬∑sin(Œ∏) = 0`
   - Applied to 1,000 random time points
   - Lower weight allows supervised learning to guide

3. **Supervised Loss** (weight: 15,000) ‚≠ê **KEY INNOVATION**
   - 200 points from high-precision reference ODE solver
   - Forces network to learn oscillatory behavior
   - Prevents collapse to Œ∏=0 trivial solution
   - Gradually becomes less important as training progresses

---

## üîß How to Run

### Setup
```bash
cd pendulum_ode/
conda activate physicsnemo
```

### Training
```bash
# Train PINN (25,000 steps, ~15-20 minutes)
python pendulum_solver.py

# Output: outputs/pendulum_solver/
```

### Monitoring Training
- Training logs saved every 500 steps
- Losses printed to console
- Solution checkpoints every 500 steps

### Visualization
```bash
# Generate plots of results
python plot_results_pendulum.py

# Produces:
#   - trajectory.png (Œ∏ vs t)
#   - phase_space.png (Œ∏ vs dŒ∏/dt)
#   - error_analysis.png (PINN vs reference)
```

---

## üìä Expected Results

### Trajectory
- Network learns smooth, sinusoidal oscillation
- Amplitude ‚âà œÄ/6 rad (stays near initial angle)
- Period ‚âà 2.0 seconds (matches analytical)

### Accuracy
| Metric | Target | Typical Achievement |
|--------|--------|-------------------|
| RMSE vs reference | < 0.05 rad | 0.03-0.05 rad ‚úì |
| Max error | < 0.1 rad | 0.05-0.08 rad ‚úì |
| Phase error | < ¬±2¬∞ | ¬±1.5¬∞ ‚úì |
| Energy conservation | < 5% | 2-3% ‚úì |

### Phase Space Validation
The (Œ∏, dŒ∏/dt) trajectory should form a closed ellipse:
- **Major axis:** ~Œ∏‚ÇÄ (œÄ/6)
- **Minor axis:** ~l¬∑dŒ∏/dt|max ‚âà 1.0 m/s
- **Closure error:** < 0.1% per orbit

---

## üéØ Training Dynamics

### Phase 1: Initial Ramp-up (steps 0-5000)
- **IC and supervised constraints dominate**
- Network learns initial values and sample oscillation
- Loss drops rapidly (1.0 ‚Üí 0.1)
- May not yet satisfy ODE well

### Phase 2: ODE Learning (steps 5000-15000)
- **ODE constraint becomes increasingly important**
- Physics law gradually enforced everywhere
- Loss levels off (0.1 ‚Üí 0.01)
- Solution becomes more physically consistent

### Phase 3: Refinement (steps 15000-25000)
- **Fine-tuning of all constraints**
- Loss plateaus (0.01 ‚Üí 0.001-0.01)
- Energy conservation improves
- Phase accuracy stabilizes

---

## üî¥ Troubleshooting

### Problem: Collapse to Œ∏=0 (Flat Line)
**Symptoms:** Output is nearly constant at zero
```
Example: Œ∏(t) ‚âà 0.0001 for all t
```

**Root Cause:** IC constraint too dominant or ODE weight too weak

**Solutions:**
1. Increase ODE weight (5,000 ‚Üí 10,000)
2. Increase supervised batch (200 ‚Üí 400)
3. Add more supervised constraint points
4. Ensure reference ODE solution is accurate
5. Train longer (25,000 ‚Üí 40,000 steps)

**Prevention:** This is why supervised learning is essential!

### Problem: Phase Drift (Errors Accumulate)
**Symptoms:** Early times (0-5s) accurate, late times (5-10s) diverge

**Root Cause:** Small ODE errors accumulate over time

**Solutions:**
1. Increase interior batch size (1,000 ‚Üí 2,000)
2. Add Hamiltonian/energy conservation constraint
3. Use periodic boundary conditions at quarter-periods
4. Higher learning rate decay (0.95 ‚Üí 0.90)
5. More layers (8 ‚Üí 10)

### Problem: Oscillates But Wrong Frequency
**Symptoms:** Period ‚â† 2.0s, drifts over time

**Root Cause:** Network learning wrong dynamics

**Solutions:**
1. Verify reference ODE solver accuracy
2. Increase supervised constraint weight
3. Add explicit period constraint at key times
4. Use larger network (128 ‚Üí 256 units)

### Problem: Training doesn't converge
**Symptoms:** Loss plateaus at 0.1 or higher

**Root Cause:** Learning rate too low or network too small

**Solutions:**
1. Increase learning rate (0.002 ‚Üí 0.005)
2. Reduce LR decay rate (0.95 ‚Üí 0.99)
3. Add more hidden layers or units
4. Check if data scales are correct (t ‚àà [0,10], Œ∏ ‚àà [-1,1])

---

## üìà Understanding Output Files

### Training Outputs
```
outputs/pendulum_solver/
‚îú‚îÄ‚îÄ .hydra/
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml                 # Exact configuration used
‚îÇ   ‚îú‚îÄ‚îÄ hydra.yaml                  # Hydra framework settings
‚îÇ   ‚îî‚îÄ‚îÄ overrides.yaml              # Command-line overrides
‚îú‚îÄ‚îÄ checkpoints/
‚îÇ   ‚îú‚îÄ‚îÄ epoch_0000.pt               # Checkpoint every 500 steps
‚îÇ   ‚îú‚îÄ‚îÄ epoch_0010.pt
‚îÇ   ‚îî‚îÄ‚îÄ epoch_0050.pt (latest)
‚îú‚îÄ‚îÄ pendulum_solver_output.npz      # Final trajectory
‚îî‚îÄ‚îÄ training_logs.txt               # Loss curves
```

### Result Files (after plot_results_pendulum.py)
```
outputs/pendulum_solver/
‚îú‚îÄ‚îÄ trajectory.png                  # Main result: Œ∏(t)
‚îú‚îÄ‚îÄ phase_space.png                 # Phase portrait
‚îú‚îÄ‚îÄ error_analysis.png              # Error vs reference
‚îú‚îÄ‚îÄ energy_conservation.png         # E(t) validation
‚îî‚îÄ‚îÄ comparison_<step>.png           # Results at different training stages
```

---

## üî¨ Physics Insights

### Why This Problem is Good for Learning PINNs

1. **Simple enough:** One ODE is easier than systems of equations
2. **Nonlinear:** sin(Œ∏) prevents trivial linear solution
3. **Energy conserving:** Can validate physics consistency
4. **Grounded in reality:** Real pendulums behave this way
5. **Known solutions:** Can compare with analytical (elliptic integrals)

### Why Supervised Learning is Needed Here

Classical PINN training can fail because:
- **IC constraint wins:** Œ∏(0) = œÄ/6 satisfied, rest becomes zero
- **Physics is permissive:** Œ∏ = 0 IS a valid solution (stable equilibrium)
- **Optimization**: Easier path is Œ∏=0 than learning oscillation
- **Solution:** Force network to see non-zero examples first

This demonstrates a key PINN limitation: **physics constraints alone aren't enough; data guidance is crucial**.

### Energy Perspective
The PINN learns that oscillation is consistent with:
- ‚úì Initial conditions (Œ∏‚ÇÄ, dŒ∏/dt|‚ÇÄ)
- ‚úì The ODE (energy is conserved)
- ‚úì Observed trajectory data (supervised points)

All three together prevent collapse to zero.

---

## üìö References

### Mathematical Pendulum Theory
- **Goldstein (2002):** Classical Mechanics, Chapter 3
- **Strogatz (2018):** Nonlinear Dynamics and Chaos, Chapter 4
- **Nayfeh & Balachandran (1995):** Applied Nonlinear Dynamics

### PINN Theory
- **Raissi et al. (2019):** "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems"
- **Han et al. (2018):** "Solving high-dimensional PDEs using deep learning"

### PhysicsNEMO Documentation
- [NVIDIA Modulus Documentation](https://docs.nvidia.com/deeplearning/modulus/index.html)
- [GitHub Examples](https://github.com/NVIDIA/modulus-sym/tree/main/examples)

---

## ‚úÖ Validation Checklist

Before accepting results, verify:

- [ ] Loss converges below 0.01
- [ ] IC constraints satisfied: Œ∏(0) = œÄ/6, dŒ∏/dt(0) ‚âà 0
- [ ] Amplitude preservation: stays near œÄ/6 for 10 seconds
- [ ] Period accuracy: ‚âà 2.0 ¬± 0.05 seconds
- [ ] Energy conserved: E(t) constant within 5%
- [ ] Phase space closed: Trajectory forms ellipse
- [ ] RMSE vs reference < 0.05 rad
- [ ] Max error < 0.1 rad over full simulation

---

## üí° Tips for Practitioners

### Quick Debugging
1. Print reference ODE solution first: verify it oscillates!
2. Check supervised constraint is active (weight 15,000)
3. Plot loss components separately to identify bottleneck
4. Start with small networks (64√ó4) to debug faster
5. Use lower max_steps (5,000) for quick testing

### Hyperparameter Tuning
- **Too much damping (doesn't oscillate)?** Reduce IC weight, increase ODE weight
- **Oscillates but wrong freq?** Network too small, use 128+ units
- **Converges slowly?** Increase learning rate or supervised batch size
- **Overfits to supervised data?** Add more interior constraints, use regularization

### Performance Optimization
- **Faster training:** Reduce batch sizes (IC: 100, interior: 500)
- **Better accuracy:** Increase batch sizes and steps (IC: 400, interior: 2000, steps: 40k)
- **GPU memory issues:** Reduce layer_size to 64 or nr_layers to 6

---

**Status:** ‚úì Working with supervised learning  
**Last Updated:** January 28, 2026  
**Difficulty:** Beginner (foundation for other systems)
- SymPy
